---
title: "**U.S. Police Killings from 2015**"
author: "Kemberli Jennings"
output: pdf_document
date: "2023-10-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = list(CRAN="http://cran.rstudio.com/"))
```



In 2015, The Washington Post started keeping a dataset of all the police killings in the United States since the police agencies weren't very diligent in keeping the dataset up to date. 

The first iteration of the Washington Post's Police Killings data didn't include the 'explanatory' columns the newest dataset has. The latest iteration now looks at more unique variables that might add new insights to an individual ending up on this list who isn't in a policing entity. The single dataset used earlier is now two datasets. I could have used Excel for such a small dataset but used SQL to merge the tables. 

The following is the SQL used to bring the two .csv tables together for the second iteration. I've since learned there's a way to eliminate going to an outside source to do SQL when using R, but there needed to be a hookup for BigQuery. This created file comes out of BigQuery as a .csv, and I used R for data cleaning instead of cleaning in an Excel file due to how the Excel file comes from BigQuery.    

*SELECT k.date, k.threat_type, k.flee_status, k.armed_with, k.city, k.state, k.name, k.age, k.gender, k.race, k.was_mental_illness_related, k.body_camera, k.agency_ids, a.agency_name, a.type
FROM my-project-052823.PoliceKillings.PoliceKillings k
JOIN my-project-052823.PoliceAgencies.PoliceAgencies a
ON k.agency_ids = a.id AND k.state = a.state
order by k.date;*      

Next is my R code. I was excited about this dataset, believing I'd find something 'new' about this problem. All of the usual comments/beliefs are out there and are well known. Plus, The Washington Post does an excellent job of putting all the known variables attached to some statistics in a visualization for everyone interested. I wasn't trying to repeat what they had done. 

```{r}

install.packages("janitor")
library(janitor)
install.packages("here")
library(here)
install.packages("skimr")
library(skimr)
install.packages("tidyverse")
library(tidyverse)
install.packages("dplyr")
library(dplyr)

install.packages("readr")
library(readr)
#install.packages("readxl")
#library(readxl)

#for date manipulations
install.packages("lubridate")
library(lubridate)
install.packages("parsedate")
library(parsedate)
install.packages("zoo")
library(zoo)

#needed to install ggpubr for correlation test
install.packages("ggpubr")
library(ggpubr)

#loading vcd package for correlation between non-numeric variables
install.packages("libgfortran")
install.packages("libquadmath")
install.packages("vcd")
library(vcd)

killings_df <- read_csv("SQLQuery_PoliceKillings.csv")

#checking for blank rows
x <- subset(killings_df, !complete.cases(killings_df))

#filling in the blanks
killings_df[killings_df==''] <- "undisclosed"

#changing yyyy-mm-dd to yyyy-mm
my_date <- format(as.Date(killings_df$date, "%d/%m/%Y"), "%Y-%m")

#add column called 'my_date'
killings_df$my_date <- my_date

```
**I'm working on the second iteration**

Since there are 3 more columns, I’d need to write a function to run the assocstats through the dataset’s variables without my human interaction. But, using the old dataset, I came up with pairs that I thought would interact together and give that new insight into these killings, which is expressed in the upcoming programming.

```{r}
#preparing to look for correlation between non-numeric variables
# Create a 2x2 contingency table
mytable_armrace <- table(killings_df$armed_with, killings_df$race)
mytable_fleerace <- table(killings_df$flee_status, killings_df$race)
mytable_staterace <- table(killings_df$state, killings_df$race)
mytable_camerapolice <- table(killings_df$agency_name, killings_df$body_camera)
mytable_racecamera <- table(killings_df$race, killings_df$body_camera)
mytable_racecity <- table(killings_df$race, killings_df$city)
glimpse(mytable_racecity)
mytable_cameracity <- table(killings_df$body_camera, killings_df$city)
mytable_policecity <- table(killings_df$agency_name, killings_df$city)

# Calculate stats associated with character vectors
assocstats(mytable_armrace)
assocstats(mytable_fleerace)
assocstats(mytable_staterace)
assocstats(mytable_camerapolice)
assocstats(mytable_racecamera)

assocstats(mytable_racecity)
assocstats(mytable_cameracity)
assocstats(mytable_policecity)

```

The results of the correlation tests weren't very heartening. By using Cramer's V:  

    * police & city --  .8161699
    * camera & city --  .6189389  

The pair that showed significant correlation at 82% per Cramer's V, police and city, will have to be examined more to determine why that is so high. The surprising pair, camera and city at 62%, didn't show such a high correlation as the first pair but was surprising since 85% of the agencies do not use cameras!

This is from the old dataset. The new dataset has added fields that further explains of the story of how that non-police individual ended up on this list. I'm looking forward to working with the new dataset to see if it will reveal something new.

What I was intending on doing was finishing with the data I curated from The Census Bureau complimenting the data I'm working with here, *if* it would further explain something new.

*The percentage of the cameras used was taken off of my Pivot Table information from the dataset.*

*This paper was done in RMarkdown*



